Warning: 3 rows excluded due to sparse bins.
Train Language(s): ['en', 'af']
Train Set: ('Len: 19997', 'Tokens: 172095', 'Unique: 35989', "Entities: {'O': 96452, 'I-ORG': 22184, 'I-PER': 14053, 'I-LOC': 11239, 'B-ORG': 9899, 'B-LOC': 9437, 'B-PER': 8831}")
Val Set: ('Len: 5000', 'Tokens: 43111', 'Unique: 13339', "Entities: {'O': 24409, 'I-ORG': 5550, 'I-PER': 3423, 'I-LOC': 2673, 'B-ORG': 2489, 'B-LOC': 2364, 'B-PER': 2203}")
Test Set(en): ('Len: 10001', 'Tokens: 80535', 'Unique: 20221', "Entities: {'O': 40875, 'I-ORG': 11638, 'I-PER': 7520, 'I-LOC': 6357, 'B-LOC': 4834, 'B-ORG': 4676, 'B-PER': 4635}")
Test Set(it): ('Len: 10000', 'Tokens: 80005', 'Unique: 18461', "Entities: {'O': 47357, 'I-ORG': 8992, 'I-PER': 5863, 'B-PER': 4911, 'B-LOC': 4600, 'I-LOC': 4166, 'B-ORG': 4116}")
Test Set(af): ('Len: 1000', 'Tokens: 10894', 'Unique: 3404', "Entities: {'O': 7764, 'I-ORG': 911, 'B-ORG': 583, 'I-PER': 548, 'B-LOC': 529, 'B-PER': 370, 'I-LOC': 189}")
Test Set(sw): ('Len: 1000', 'Tokens: 5609', 'Unique: 1830', "Entities: {'O': 2412, 'I-ORG': 696, 'I-PER': 674, 'I-LOC': 618, 'B-LOC': 452, 'B-PER': 403, 'B-ORG': 354}")
Test Set(de): ('Len: 10000', 'Tokens: 97805', 'Unique: 23570', "Entities: {'O': 69057, 'I-PER': 6539, 'I-ORG': 6107, 'B-LOC': 4968, 'B-PER': 4569, 'B-ORG': 4281, 'I-LOC': 2284}")
Trainable parameters:  278,224,135
[[0.2914946000477311], [0.31624518675259505], [0.26015812557297774], [0.16863601142540574], [0.8784852232784033], [0.2339933193482149]]
0.2914946000477311
Epoch 1/20, train_loss: 0.3270, avg_val_loss: 0.2915
[[0.2914946000477311, 0.24968471484863833], [0.31624518675259505, 0.2721667118584767], [0.26015812557297774, 0.22945924287739272], [0.16863601142540574, 0.137065238552168], [0.8784852232784033, 0.8891343623399734], [0.2339933193482149, 0.2261283647852203]]
0.24968471484863833
Epoch 2/20, train_loss: 0.3110, avg_val_loss: 0.2497
[[0.2914946000477311, 0.24968471484863833, 0.24965148813025967], [0.31624518675259505, 0.2721667118584767, 0.28005980295590316], [0.26015812557297774, 0.22945924287739272, 0.24988882340038546], [0.16863601142540574, 0.137065238552168, 0.13685601449105889], [0.8784852232784033, 0.8891343623399734, 0.9156863959506154], [0.2339933193482149, 0.2261283647852203, 0.22366621181035576]]
0.24965148813025967
Epoch 3/20, train_loss: 0.2052, avg_val_loss: 0.2497
[[0.2914946000477311, 0.24968471484863833, 0.24965148813025967, 0.2475099191781442], [0.31624518675259505, 0.2721667118584767, 0.28005980295590316, 0.26893102455015383], [0.26015812557297774, 0.22945924287739272, 0.24988882340038546, 0.24977479553736817], [0.16863601142540574, 0.137065238552168, 0.13685601449105889, 0.13818283390719444], [0.8784852232784033, 0.8891343623399734, 0.9156863959506154, 0.8983861301094294], [0.2339933193482149, 0.2261283647852203, 0.22366621181035576, 0.23528121673641875]]
0.2475099191781442
Epoch 4/20, train_loss: 0.2764, avg_val_loss: 0.2475
[[0.2914946000477311, 0.24968471484863833, 0.24965148813025967, 0.2475099191781442, 0.2385781538809181], [0.31624518675259505, 0.2721667118584767, 0.28005980295590316, 0.26893102455015383, 0.2676997709864625], [0.26015812557297774, 0.22945924287739272, 0.24988882340038546, 0.24977479553736817, 0.24677293198986555], [0.16863601142540574, 0.137065238552168, 0.13685601449105889, 0.13818283390719444, 0.11514815699774772], [0.8784852232784033, 0.8891343623399734, 0.9156863959506154, 0.8983861301094294, 1.018139598891139], [0.2339933193482149, 0.2261283647852203, 0.22366621181035576, 0.23528121673641875, 0.23020237265303492]]
0.2385781538809181
Epoch 5/20, train_loss: 0.1841, avg_val_loss: 0.2386
[[0.2914946000477311, 0.24968471484863833, 0.24965148813025967, 0.2475099191781442, 0.2385781538809181, 0.2498305270531375], [0.31624518675259505, 0.2721667118584767, 0.28005980295590316, 0.26893102455015383, 0.2676997709864625, 0.27017535608464155], [0.26015812557297774, 0.22945924287739272, 0.24988882340038546, 0.24977479553736817, 0.24677293198986555, 0.25538761129442117], [0.16863601142540574, 0.137065238552168, 0.13685601449105889, 0.13818283390719444, 0.11514815699774772, 0.14834344934206456], [0.8784852232784033, 0.8891343623399734, 0.9156863959506154, 0.8983861301094294, 1.018139598891139, 1.0232034139335155], [0.2339933193482149, 0.2261283647852203, 0.22366621181035576, 0.23528121673641875, 0.23020237265303492, 0.25001914116045154]]
0.2498305270531375
Epoch 6/20, train_loss: 0.2015, avg_val_loss: 0.2498
[[0.2914946000477311, 0.24968471484863833, 0.24965148813025967, 0.2475099191781442, 0.2385781538809181, 0.2498305270531375, 0.2429636992798869], [0.31624518675259505, 0.2721667118584767, 0.28005980295590316, 0.26893102455015383, 0.2676997709864625, 0.27017535608464155, 0.26626886560108526], [0.26015812557297774, 0.22945924287739272, 0.24988882340038546, 0.24977479553736817, 0.24677293198986555, 0.25538761129442117, 0.27203434343321825], [0.16863601142540574, 0.137065238552168, 0.13685601449105889, 0.13818283390719444, 0.11514815699774772, 0.14834344934206456, 0.12511660839663818], [0.8784852232784033, 0.8891343623399734, 0.9156863959506154, 0.8983861301094294, 1.018139598891139, 1.0232034139335155, 1.122117541730404], [0.2339933193482149, 0.2261283647852203, 0.22366621181035576, 0.23528121673641875, 0.23020237265303492, 0.25001914116045154, 0.26332493845266275]]
0.2429636992798869
Epoch 7/20, train_loss: 0.1752, avg_val_loss: 0.2430
[[0.2914946000477311, 0.24968471484863833, 0.24965148813025967, 0.2475099191781442, 0.2385781538809181, 0.2498305270531375, 0.2429636992798869, 0.2627893953471427], [0.31624518675259505, 0.2721667118584767, 0.28005980295590316, 0.26893102455015383, 0.2676997709864625, 0.27017535608464155, 0.26626886560108526, 0.2820506596122496], [0.26015812557297774, 0.22945924287739272, 0.24988882340038546, 0.24977479553736817, 0.24677293198986555, 0.25538761129442117, 0.27203434343321825, 0.28546279259383106], [0.16863601142540574, 0.137065238552168, 0.13685601449105889, 0.13818283390719444, 0.11514815699774772, 0.14834344934206456, 0.12511660839663818, 0.1484935722546652], [0.8784852232784033, 0.8891343623399734, 0.9156863959506154, 0.8983861301094294, 1.018139598891139, 1.0232034139335155, 1.122117541730404, 1.143394535407424], [0.2339933193482149, 0.2261283647852203, 0.22366621181035576, 0.23528121673641875, 0.23020237265303492, 0.25001914116045154, 0.26332493845266275, 0.27014962626627076]]
0.2627893953471427
Epoch 8/20, train_loss: 0.1539, avg_val_loss: 0.2628
Early stopping at epoch 8 as avg validation loss has not decreased for 3 epochs since epoch-4.
Test Set(en): ('Len: 10001', 'Tokens: 80535', 'Unique: 20221', "Entities: {'O': 40875, 'I-ORG': 11638, 'I-PER': 7520, 'I-LOC': 6357, 'B-LOC': 4834, 'B-ORG': 4676, 'B-PER': 4635}")
Test Set(it): ('Len: 10000', 'Tokens: 80005', 'Unique: 18461', "Entities: {'O': 47357, 'I-ORG': 8992, 'I-PER': 5863, 'B-PER': 4911, 'B-LOC': 4600, 'I-LOC': 4166, 'B-ORG': 4116}")
Test Set(af): ('Len: 1000', 'Tokens: 10894', 'Unique: 3404', "Entities: {'O': 7764, 'I-ORG': 911, 'B-ORG': 583, 'I-PER': 548, 'B-LOC': 529, 'B-PER': 370, 'I-LOC': 189}")
Test Set(sw): ('Len: 1000', 'Tokens: 5609', 'Unique: 1830', "Entities: {'O': 2412, 'I-ORG': 696, 'I-PER': 674, 'I-LOC': 618, 'B-LOC': 452, 'B-PER': 403, 'B-ORG': 354}")
Test Set(de): ('Len: 10000', 'Tokens: 97805', 'Unique: 23570', "Entities: {'O': 69057, 'I-PER': 6539, 'I-ORG': 6107, 'B-LOC': 4968, 'B-PER': 4569, 'B-ORG': 4281, 'I-LOC': 2284}")
LANG [en]: F1-score (Token-level) : 0.845
Test Loss: 0.2820506596122496 
 Test Evaluation (strict IOB2):
              precision    recall  f1-score   support

         LOC      0.876     0.888     0.882      9463
         ORG      0.793     0.745     0.768      7848
         PER      0.894     0.905     0.899      7491

   micro avg      0.857     0.848     0.852     24802
   macro avg      0.854     0.846     0.850     24802
weighted avg      0.855     0.848     0.851     24802

LANG [it]: F1-score (Token-level) : 0.854
Test Loss: 0.28546279259383106 
 Test Evaluation (strict IOB2):
              precision    recall  f1-score   support

         LOC      0.853     0.807     0.829      9427
         ORG      0.798     0.806     0.802      7224
         PER      0.944     0.947     0.946      8423

   micro avg      0.868     0.854     0.861     25074
   macro avg      0.865     0.853     0.859     25074
weighted avg      0.868     0.854     0.860     25074

LANG [af]: F1-score (Token-level) : 0.897
Test Loss: 0.1484935722546652 
 Test Evaluation (strict IOB2):
              precision    recall  f1-score   support

         LOC      0.856     0.920     0.887      1296
         ORG      0.889     0.882     0.885      1304
         PER      0.940     0.961     0.950       666

   micro avg      0.886     0.913     0.899      3266
   macro avg      0.895     0.921     0.907      3266
weighted avg      0.886     0.913     0.899      3266

LANG [sw]: F1-score (Token-level) : 0.672
Test Loss: 1.143394535407424 
 Test Evaluation (strict IOB2):
              precision    recall  f1-score   support

         LOC      0.618     0.785     0.692       908
         ORG      0.640     0.358     0.459       701
         PER      0.868     0.840     0.853       617

   micro avg      0.692     0.666     0.679      2226
   macro avg      0.709     0.661     0.668      2226
weighted avg      0.694     0.666     0.663      2226

LANG [de]: F1-score (Token-level) : 0.801
Test Loss: 0.27014962626627076 
 Test Evaluation (strict IOB2):
              precision    recall  f1-score   support

         LOC      0.820     0.831     0.825     11290
         ORG      0.693     0.740     0.716      9151
         PER      0.926     0.854     0.889      7933

   micro avg      0.804     0.808     0.806     28374
   macro avg      0.813     0.808     0.810     28374
weighted avg      0.809     0.808     0.808     28374

Time Elapsed:  3852.278035879135

Warning: 3 rows excluded due to sparse bins.
Train Language(s): ['en', 'hu', 'af']
Train Set: ('Len: 35997', 'Tokens: 316570', 'Unique: 66200', "Entities: {'O': 196715, 'I-ORG': 31209, 'I-PER': 22766, 'B-LOC': 18645, 'B-ORG': 16250, 'B-PER': 15918, 'I-LOC': 15067}")
Val Set: ('Len: 9000', 'Tokens: 79095', 'Unique: 24501', "Entities: {'O': 48966, 'I-ORG': 8003, 'I-PER': 5688, 'B-LOC': 4647, 'B-ORG': 4208, 'B-PER': 3987, 'I-LOC': 3596}")
Test Set(en): ('Len: 10001', 'Tokens: 80535', 'Unique: 20221', "Entities: {'O': 40875, 'I-ORG': 11638, 'I-PER': 7520, 'I-LOC': 6357, 'B-LOC': 4834, 'B-ORG': 4676, 'B-PER': 4635}")
Test Set(it): ('Len: 10000', 'Tokens: 80005', 'Unique: 18461', "Entities: {'O': 47357, 'I-ORG': 8992, 'I-PER': 5863, 'B-PER': 4911, 'B-LOC': 4600, 'I-LOC': 4166, 'B-ORG': 4116}")
Test Set(af): ('Len: 1000', 'Tokens: 10894', 'Unique: 3404', "Entities: {'O': 7764, 'I-ORG': 911, 'B-ORG': 583, 'I-PER': 548, 'B-LOC': 529, 'B-PER': 370, 'I-LOC': 189}")
Test Set(sw): ('Len: 1000', 'Tokens: 5609', 'Unique: 1830', "Entities: {'O': 2412, 'I-ORG': 696, 'I-PER': 674, 'I-LOC': 618, 'B-LOC': 452, 'B-PER': 403, 'B-ORG': 354}")
Test Set(de): ('Len: 10000', 'Tokens: 97805', 'Unique: 23570', "Entities: {'O': 69057, 'I-PER': 6539, 'I-ORG': 6107, 'B-LOC': 4968, 'B-PER': 4569, 'B-ORG': 4281, 'I-LOC': 2284}")
Trainable parameters:  278,224,135
[[0.1966587365704014], [0.30695145708113053], [0.23818383220666514], [0.1546866927528754], [0.8495373595505953], [0.22382230854358154]]
0.1966587365704014
Epoch 1/20, train_loss: 0.4006, avg_val_loss: 0.1967
[[0.1966587365704014, 0.18059296653018142], [0.30695145708113053, 0.28736784257970677], [0.23818383220666514, 0.2412339458450342], [0.1546866927528754, 0.143812874564901], [0.8495373595505953, 0.8849444352090359], [0.22382230854358154, 0.22495495179257455]]
0.18059296653018142
Epoch 2/20, train_loss: 0.1369, avg_val_loss: 0.1806
[[0.1966587365704014, 0.18059296653018142, 0.1666152322506334], [0.30695145708113053, 0.28736784257970677, 0.2671500442388911], [0.23818383220666514, 0.2412339458450342, 0.23256154288165867], [0.1546866927528754, 0.143812874564901, 0.1282519338419661], [0.8495373595505953, 0.8849444352090359, 0.8371892189607024], [0.22382230854358154, 0.22495495179257455, 0.2069907137951531]]
0.1666152322506334
Epoch 3/20, train_loss: 0.2422, avg_val_loss: 0.1666
[[0.1966587365704014, 0.18059296653018142, 0.1666152322506334, 0.1730994803381833], [0.30695145708113053, 0.28736784257970677, 0.2671500442388911, 0.2712342384310005], [0.23818383220666514, 0.2412339458450342, 0.23256154288165867, 0.24435854351487213], [0.1546866927528754, 0.143812874564901, 0.1282519338419661, 0.13393526250729337], [0.8495373595505953, 0.8849444352090359, 0.8371892189607024, 0.917381763458252], [0.22382230854358154, 0.22495495179257455, 0.2069907137951531, 0.22752343107479067]]
0.1730994803381833
Epoch 4/20, train_loss: 0.4266, avg_val_loss: 0.1731
[[0.1966587365704014, 0.18059296653018142, 0.1666152322506334, 0.1730994803381833, 0.1629915866498829], [0.30695145708113053, 0.28736784257970677, 0.2671500442388911, 0.2712342384310005, 0.26849534199498715], [0.23818383220666514, 0.2412339458450342, 0.23256154288165867, 0.24435854351487213, 0.24575740568482649], [0.1546866927528754, 0.143812874564901, 0.1282519338419661, 0.13393526250729337, 0.11798397509846836], [0.8495373595505953, 0.8849444352090359, 0.8371892189607024, 0.917381763458252, 1.0150506366044283], [0.22382230854358154, 0.22495495179257455, 0.2069907137951531, 0.22752343107479067, 0.22541177998811673]]
0.1629915866498829
Epoch 5/20, train_loss: 0.0726, avg_val_loss: 0.1630
[[0.1966587365704014, 0.18059296653018142, 0.1666152322506334, 0.1730994803381833, 0.1629915866498829, 0.18237438282751023], [0.30695145708113053, 0.28736784257970677, 0.2671500442388911, 0.2712342384310005, 0.26849534199498715, 0.28129456019677673], [0.23818383220666514, 0.2412339458450342, 0.23256154288165867, 0.24435854351487213, 0.24575740568482649, 0.2706166034855972], [0.1546866927528754, 0.143812874564901, 0.1282519338419661, 0.13393526250729337, 0.11798397509846836, 0.15150594478473067], [0.8495373595505953, 0.8849444352090359, 0.8371892189607024, 0.917381763458252, 1.0150506366044283, 1.0593097433447838], [0.22382230854358154, 0.22495495179257455, 0.2069907137951531, 0.22752343107479067, 0.22541177998811673, 0.24288670208078986]]
0.18237438282751023
Epoch 6/20, train_loss: 0.1288, avg_val_loss: 0.1824
[[0.1966587365704014, 0.18059296653018142, 0.1666152322506334, 0.1730994803381833, 0.1629915866498829, 0.18237438282751023, 0.17776066566824067], [0.30695145708113053, 0.28736784257970677, 0.2671500442388911, 0.2712342384310005, 0.26849534199498715, 0.28129456019677673, 0.2803192962163363], [0.23818383220666514, 0.2412339458450342, 0.23256154288165867, 0.24435854351487213, 0.24575740568482649, 0.2706166034855972, 0.2754979847362057], [0.1546866927528754, 0.143812874564901, 0.1282519338419661, 0.13393526250729337, 0.11798397509846836, 0.15150594478473067, 0.12773639184888452], [0.8495373595505953, 0.8849444352090359, 0.8371892189607024, 0.917381763458252, 1.0150506366044283, 1.0593097433447838, 1.0793754011392593], [0.22382230854358154, 0.22495495179257455, 0.2069907137951531, 0.22752343107479067, 0.22541177998811673, 0.24288670208078986, 0.24722879152661695]]
0.17776066566824067
Epoch 7/20, train_loss: 0.0386, avg_val_loss: 0.1778
[[0.1966587365704014, 0.18059296653018142, 0.1666152322506334, 0.1730994803381833, 0.1629915866498829, 0.18237438282751023, 0.17776066566824067, 0.18377833683473052], [0.30695145708113053, 0.28736784257970677, 0.2671500442388911, 0.2712342384310005, 0.26849534199498715, 0.28129456019677673, 0.2803192962163363, 0.2874057301626609], [0.23818383220666514, 0.2412339458450342, 0.23256154288165867, 0.24435854351487213, 0.24575740568482649, 0.2706166034855972, 0.2754979847362057, 0.28466582517868605], [0.1546866927528754, 0.143812874564901, 0.1282519338419661, 0.13393526250729337, 0.11798397509846836, 0.15150594478473067, 0.12773639184888452, 0.1227246685884893], [0.8495373595505953, 0.8849444352090359, 0.8371892189607024, 0.917381763458252, 1.0150506366044283, 1.0593097433447838, 1.0793754011392593, 1.0848667360842228], [0.22382230854358154, 0.22495495179257455, 0.2069907137951531, 0.22752343107479067, 0.22541177998811673, 0.24288670208078986, 0.24722879152661695, 0.26181543436341775]]
0.18377833683473052
Epoch 8/20, train_loss: 0.1199, avg_val_loss: 0.1838
Early stopping at epoch 8 as avg validation loss has not decreased for 3 epochs since epoch-4.
Test Set(en): ('Len: 10001', 'Tokens: 80535', 'Unique: 20221', "Entities: {'O': 40875, 'I-ORG': 11638, 'I-PER': 7520, 'I-LOC': 6357, 'B-LOC': 4834, 'B-ORG': 4676, 'B-PER': 4635}")
Test Set(it): ('Len: 10000', 'Tokens: 80005', 'Unique: 18461', "Entities: {'O': 47357, 'I-ORG': 8992, 'I-PER': 5863, 'B-PER': 4911, 'B-LOC': 4600, 'I-LOC': 4166, 'B-ORG': 4116}")
Test Set(af): ('Len: 1000', 'Tokens: 10894', 'Unique: 3404', "Entities: {'O': 7764, 'I-ORG': 911, 'B-ORG': 583, 'I-PER': 548, 'B-LOC': 529, 'B-PER': 370, 'I-LOC': 189}")
Test Set(sw): ('Len: 1000', 'Tokens: 5609', 'Unique: 1830', "Entities: {'O': 2412, 'I-ORG': 696, 'I-PER': 674, 'I-LOC': 618, 'B-LOC': 452, 'B-PER': 403, 'B-ORG': 354}")
Test Set(de): ('Len: 10000', 'Tokens: 97805', 'Unique: 23570', "Entities: {'O': 69057, 'I-PER': 6539, 'I-ORG': 6107, 'B-LOC': 4968, 'B-PER': 4569, 'B-ORG': 4281, 'I-LOC': 2284}")
LANG [en]: F1-score (Token-level) : 0.846
Test Loss: 0.2874057301626609 
 Test Evaluation (strict IOB2):
              precision    recall  f1-score   support

         LOC      0.875     0.897     0.886      9463
         ORG      0.785     0.750     0.767      7848
         PER      0.904     0.900     0.902      7491

   micro avg      0.856     0.852     0.854     24802
   macro avg      0.855     0.849     0.852     24802
weighted avg      0.855     0.852     0.853     24802

LANG [it]: F1-score (Token-level) : 0.856
Test Loss: 0.28466582517868605 
 Test Evaluation (strict IOB2):
              precision    recall  f1-score   support

         LOC      0.854     0.803     0.827      9427
         ORG      0.789     0.821     0.805      7224
         PER      0.952     0.945     0.948      8423

   micro avg      0.867     0.856     0.861     25074
   macro avg      0.865     0.856     0.860     25074
weighted avg      0.868     0.856     0.861     25074

LANG [af]: F1-score (Token-level) : 0.907
Test Loss: 0.1227246685884893 
 Test Evaluation (strict IOB2):
              precision    recall  f1-score   support

         LOC      0.884     0.910     0.897      1296
         ORG      0.889     0.911     0.900      1304
         PER      0.951     0.952     0.951       666

   micro avg      0.899     0.919     0.909      3266
   macro avg      0.908     0.924     0.916      3266
weighted avg      0.900     0.919     0.909      3266

LANG [sw]: F1-score (Token-level) : 0.650
Test Loss: 1.0848667360842228 
 Test Evaluation (strict IOB2):
              precision    recall  f1-score   support

         LOC      0.619     0.725     0.668       908
         ORG      0.540     0.424     0.475       701
         PER      0.842     0.820     0.831       617

   micro avg      0.660     0.656     0.658      2226
   macro avg      0.667     0.656     0.658      2226
weighted avg      0.656     0.656     0.652      2226

LANG [de]: F1-score (Token-level) : 0.819
Test Loss: 0.26181543436341775 
 Test Evaluation (strict IOB2):
              precision    recall  f1-score   support

         LOC      0.832     0.840     0.836     11290
         ORG      0.742     0.758     0.750      9151
         PER      0.935     0.848     0.890      7933

   micro avg      0.828     0.816     0.822     28374
   macro avg      0.836     0.816     0.825     28374
weighted avg      0.832     0.816     0.823     28374

Time Elapsed:  5776.234041213989

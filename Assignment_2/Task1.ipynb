{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c2f418-840c-4e02-a512-7304c9396370",
   "metadata": {
    "id": "20c2f418-840c-4e02-a512-7304c9396370"
   },
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6309e048-f299-45b3-896a-1eb178eb77f7",
   "metadata": {
    "id": "6309e048-f299-45b3-896a-1eb178eb77f7"
   },
   "outputs": [],
   "source": [
    "# from eval_on_test import read_data\n",
    "#https://huggingface.co/docs/transformers/tasks/token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a3d63b-d420-48ce-866d-b117bd95afc9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "c5a3d63b-d420-48ce-866d-b117bd95afc9",
    "outputId": "16426c52-e2b5-4c86-956d-8a77b330c7d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 20:26:45.536344: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-06 20:26:45.536413: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-06 20:26:45.542510: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-06 20:26:45.557502: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-06 20:26:47.244975: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, pipeline,  AutoModelForTokenClassification\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6fa05a5-f8fe-4ad8-a641-3d4e1d22507c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6fa05a5-f8fe-4ad8-a641-3d4e1d22507c",
    "outputId": "422ccd3a-9239-42a4-9652-c3a5269e1c3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'I-ORG': 1,\n",
       " 'I-PER': 2,\n",
       " 'I-LOC': 3,\n",
       " 'B-ORG': 4,\n",
       " 'B-PER': 5,\n",
       " 'B-LOC': 6}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {0: 'O',1: 'I-ORG', 2: 'I-PER', 3: 'I-LOC', 4: 'B-ORG', 5: 'B-PER', 6: 'B-LOC'}\n",
    "label2id = {v:i for i,v in id2label.items()}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94031e00-87d4-486c-b063-a2061e121ab0",
   "metadata": {
    "id": "94031e00-87d4-486c-b063-a2061e121ab0"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0abe3bb4-647c-40ae-a345-a82b542d25bd",
   "metadata": {
    "id": "0abe3bb4-647c-40ae-a345-a82b542d25bd"
   },
   "outputs": [],
   "source": [
    "def read_and_combine_datasets(fpaths):\n",
    "    \"\"\"\n",
    "    Read and combine datasets from given file paths.\n",
    "\n",
    "    Args:\n",
    "        fpaths (list of str): List of file paths.\n",
    "\n",
    "    Returns:\n",
    "        combined_df (DataFrame): Combined dataset.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #############################\n",
    "    # # Generate examples\n",
    "    # combined_data = []\n",
    "    # for _ in range(1000):\n",
    "    #     word_list = ['apple', 'banana', 'cat', 'dog', 'house', 'tree', 'car', 'book', 'sun', 'moon', \".\"]\n",
    "\n",
    "    #     random_word = random.choice(word_list)\n",
    "    #     # Generate a random sentence length between 3 and 10 tokens\n",
    "    #     sentence_length = random.randint(3, 10)\n",
    "    #     combined_data.extend([[random.choice(word_list),id2label[random.randint(0, 6)]] for i in  range(1, sentence_length+1)])\n",
    "    #############################\n",
    "    combined_data = []\n",
    "    for fpath in fpaths:\n",
    "        with gzip.open(fpath, 'rt', encoding='utf-8') as f:\n",
    "            data = [line.strip().split('\\t') for line in f.readlines() if line.strip()]\n",
    "            combined_data.extend(data)\n",
    "\n",
    "    df  = pd.DataFrame(combined_data, columns=[\"token\", \"label\"])\n",
    "    df['sentence_start'] = (df['token'].str.endswith('.')).shift(1, fill_value=True).cumsum()\n",
    "    sentences_df = df.groupby('sentence_start').agg({'token': list, 'label': list}).reset_index(drop=True)\n",
    "    # Rename columns to match the desired output\n",
    "    sentences_df.columns = ['sentence', 'label']\n",
    "    # sentences_df = sentences_df[sentences_df['sentence'].apply(len) >= 2]\n",
    "\n",
    "    return sentences_df\n",
    "\n",
    "def stratified_split_train_set(train_df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform a stratified split on the training set.\n",
    "\n",
    "    Args:\n",
    "        train_df (DataFrame): Training dataset.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        random_state (int): Controls the shuffling applied to the data before applying the split.\n",
    "\n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test: Split datasets.\n",
    "    \"\"\"\n",
    "    # Stratify by labels to maintain distribution\n",
    "    strats= list(pd.cut(train_df[\"sentence\"].apply(len), bins=[0, 5, 10, 15, 20, 25, 30, 55, 80, 100, 200],\n",
    "                                                                    labels=['0-5', '5-10', '10-15', '15-20', '20-25', '25-30', '30-55', '50-80', '80-100', '100-200']).values)\n",
    "    return train_test_split(train_df[\"sentence\"], train_df[\"label\"], test_size=test_size, random_state=random_state, stratify=strats)\n",
    "\n",
    "\n",
    "def analyze_dataset(dataset, labels):\n",
    "    num_examples = len(dataset)\n",
    "    num_tokens = sum(len(text) for text in dataset)\n",
    "    named_entities = dict(Counter(ent for label in labels for ent in label).most_common())\n",
    "    total_unique_words = len(set(word for sentence in dataset for word in sentence))\n",
    "    return \"Len: \"+ str(num_examples),  \"Tokens: \"+str(num_tokens), \"Unique: \"+str(total_unique_words), \"Entities: \"+ str(named_entities)\n",
    "\n",
    "\n",
    "def combine_and_split_datasets(train_langs=[\"en\", \"it\", \"af\", \"sw\", \"de\"], data_dir=\"/fp/projects01/ec30/IN5550/obligatories/2/data\"):\n",
    "    \"\"\"\n",
    "    Combine, split datasets based on specified languages, and prepare training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        train_langs (list of str): Languages for training data.\n",
    "        val_langs (list of str): Languages for validation data.\n",
    "        data_dir (str): Directory where the datasets are stored.\n",
    "\n",
    "    Returns:\n",
    "        training_set, X_train, X_test, y_train, y_test, validation_set: Prepared datasets.\n",
    "    \"\"\"\n",
    "    # Combine training datasets\n",
    "    train_paths = [f\"{data_dir}/train-{lang}.tsv.gz\" for lang in train_langs]\n",
    "    training_set = read_and_combine_datasets(train_paths)\n",
    "\n",
    "    # Stratified split on training set\n",
    "    _X_train, _X_val, _y_train, _y_val = stratified_split_train_set(training_set)\n",
    "    print(\"Train Language(s):\", train_langs)\n",
    "    print(\"Train Set:\", analyze_dataset(_X_train,_y_train ))\n",
    "    print(\"Val Set:\",  analyze_dataset(_X_val, _y_val))\n",
    "\n",
    "    return list(_X_train), list(_X_val) , list(_y_train), list(_y_val)\n",
    "\n",
    "def getTestDatasets(val_langs=[\"en\"], data_dir=\"/fp/projects01/ec30/IN5550/obligatories/2/data\"):\n",
    "    all_set ={}\n",
    "    for lang in val_langs:\n",
    "        val_set = read_and_combine_datasets([f\"{data_dir}/dev-{lang}.tsv.gz\"])\n",
    "        _X_test,_y_test  = list(val_set[\"sentence\"]), list(val_set[\"label\"])\n",
    "        print(f\"Test Set({lang}):\",  analyze_dataset(_X_test,_y_test))\n",
    "        all_set[lang] = _X_test,_y_test\n",
    "    return all_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4c850424-46f4-4006-b4c7-6be5d8b4f58e",
   "metadata": {
    "id": "4c850424-46f4-4006-b4c7-6be5d8b4f58e"
   },
   "outputs": [],
   "source": [
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, label2id):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = tokenizer.model_max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        word_labels = self.labels[idx]\n",
    "\n",
    "        tokenized_inputs = self.tokenizer(text, is_split_into_words=True, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "        input_ids = tokenized_inputs['input_ids'].squeeze()\n",
    "        attention_mask = tokenized_inputs['attention_mask'].squeeze()\n",
    "\n",
    "        labels = []\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=0)  # Get word_ids for the current sequence\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:  # Special tokens\n",
    "                labels.append(-100)  # PyTorch's convention to ignore these labels during loss calculation\n",
    "            elif word_idx != previous_word_idx:  # New word\n",
    "                labels.append(self.label2id[word_labels[word_idx]])\n",
    "            else:  # Subtoken of the previous word\n",
    "                labels.append(self.label2id[word_labels[word_idx]])\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        label_tensor = torch.LongTensor(labels)\n",
    "\n",
    "        # Ensure label_tensor is padded/truncated to the correct length\n",
    "        # This might not be necessary if your tokenizer already ensures correct padding\n",
    "        # but is here for safety\n",
    "        # padded_label_tensor = torch.full((self.max_length,), fill_value=-100, dtype=torch.long)\n",
    "        # padded_label_tensor[:len(labels)] = label_tensor[:self.max_length]\n",
    "\n",
    "        return input_ids, attention_mask, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d1a6ec-6b1b-4479-af3d-782aa410472a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "nQK0gVzNwWrq",
   "metadata": {
    "id": "nQK0gVzNwWrq"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_token_id):\n",
    "    input_ids, attention_masks, labels = zip(*batch)\n",
    "\n",
    "    # Pad sequences so they match the longest sequence in the batch\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=pad_token_id)\n",
    "    attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Assuming labels are already tensors. If not, you might need to convert or pad them here\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100) # Use -100 for ignored index in CrossEntropyLoss\n",
    "\n",
    "    return input_ids_padded.to(device), attention_masks_padded.to(device), labels_padded.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cqjRfYj3EzvQ",
   "metadata": {
    "id": "cqjRfYj3EzvQ"
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "def validate(model, data_loader, evaluate=False,lang=\"en\"):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids, atn_mask, labels = batch\n",
    "            outputs = model(input_ids=input_ids, attention_mask=atn_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            prediction = outputs.logits.argmax(dim=2).cpu().numpy()\n",
    "            \n",
    "            cropped_predictions = [pred[1:len(mask) - 1 - mask.flip(dims=[0]).argmax()] for pred, mask in zip(prediction, atn_mask)]\n",
    "            cropped_true_labels = [true[1:len(mask) - 1 - mask.flip(dims=[0]).argmax()] for true, mask in zip(labels.cpu().numpy(), atn_mask)]\n",
    "\n",
    "            predictions.extend(cropped_predictions)\n",
    "            true_labels.extend(cropped_true_labels)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    # Convert predictions and true labels to tag sequences\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    pred_tags = [[id2label[tag_id] for tag_id in seq] for seq in predictions]\n",
    "    true_tags = [[id2label[tag_id] if tag_id in id2label else '-100' for tag_id in seq] for seq in true_labels]\n",
    "    f1_score_ = f1_score(true_tags, pred_tags)\n",
    "    if evaluate:\n",
    "        print(f\"LANG [{lang}]: F1-score (Token-level) : {f1_score_:.3f}\")\n",
    "        print(f'Test Loss: {test_loss} \\n Test Evaluation (strict IOB2):')\n",
    "        print(classification_report(true_tags, pred_tags, zero_division=0, mode='strict', scheme=IOB2, digits=3))\n",
    "    return avg_loss, true_tags, pred_tags, f1_score_\n",
    "\n",
    "\n",
    "def plot_loss(train_losses, val_losses, f1_scores):\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, marker='o', label='Validation Loss')\n",
    "    plt.plot(range(1, len(f1_scores) + 1), f1_scores, marker='.', label='Val. F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=3, lr=5e-5, early_stopping_patience=3, finetune=False):\n",
    "    model.requires_grad_(finetune)\n",
    "    model.classifier.requires_grad_(True)\n",
    "    model.train()\n",
    "    print(\"Trainable parameters: \", format(sum(p.numel() for p in model.parameters() if p.requires_grad), \",\"))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_f1s =[]\n",
    "    \n",
    "    best_val_loss, best_epoch = float('inf'), 0\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_state_dict = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Training\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', leave=False)\n",
    "        for batch in progress_bar:\n",
    "            model.train()\n",
    "            input_ids, atn_mask, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=atn_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'train_loss': loss.item()})\n",
    "        train_losses.append(total_train_loss / len(train_loader))\n",
    "\n",
    "        # Validation\n",
    "        val_loss, true_tags, pred_tags, f1_score = validate(model, val_loader)\n",
    "        val_f1s.append(f1_score)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, train_loss : {loss.item():.4f}, val_loss: {val_loss:.4f}, val_f1: {f1_score:.4f}')\n",
    "        scheduler.step(val_loss)\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss, best_epoch = val_loss, epoch\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_state_dict = model.state_dict()\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= early_stopping_patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1} as validation loss has not decreased for {early_stopping_patience} epochs since epoch-{best_epoch}.')\n",
    "                break\n",
    "\n",
    "    # Load the best model state\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "\n",
    "    plot_loss(train_losses, val_losses,val_f1s )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "p5QFHV9qI5Dw",
   "metadata": {
    "id": "p5QFHV9qI5Dw"
   },
   "outputs": [],
   "source": [
    "# model_path = \"/fp/projects01/ec30/models/xlm-roberta-base/\"\n",
    "# # \"/fp/projects01/ec30/models/bert-base-multilingual-cased/\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, model_max_length=256)\n",
    "# model = AutoModelForTokenClassification.from_pretrained(model_path, num_labels=len(label2id)).to(device)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = combine_and_split_datasets([\"en\"])\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\", model_max_length=128)\n",
    "# # model = AutoModelForTokenClassification.from_pretrained(\"google-bert/bert-base-multilingual-cased\", num_labels=len(label2id)).to(device)\n",
    "# train_loader = DataLoader(NERDataset(X_train, y_train, tokenizer, label2id), batch_size=32, shuffle=True, collate_fn=partial(collate_fn, pad_token_id =tokenizer.pad_token_id))\n",
    "# val_loader = DataLoader(NERDataset(X_val, y_val, tokenizer, label2id), batch_size=32, shuffle=False, collate_fn=partial(collate_fn, pad_token_id =tokenizer.pad_token_id))\n",
    "# _model = train(model, train_loader, val_loader, epochs=10, lr=1e-5, finetune=True)\n",
    "\n",
    "# for lang, data in getTestDatasets([\"en\", \"de\"]).items():\n",
    "#     X_test, y_test = data\n",
    "#     test_loader = DataLoader(NERDataset(X_test, y_test, tokenizer, label2id), batch_size=32, shuffle=False, collate_fn=partial(collate_fn, pad_token_id =tokenizer.pad_token_id))\n",
    "#     test_loss, true_tags, pred_tags, _ = validate(model, test_loader, evaluate=True, lang=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "058fd2f1-a725-4991-be99-55a9a6ed4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "def train_test(train_langs, val_langs, model_path, epochs=50, lr=1e-5, bs=32, finetune=True, model_max_length=256):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, model_max_length=model_max_length)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path, num_labels=len(label2id)).to(device)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = combine_and_split_datasets(train_langs)\n",
    "    train_loader = DataLoader(NERDataset(X_train, y_train, tokenizer, label2id), batch_size=bs, shuffle=True, collate_fn=partial(collate_fn, pad_token_id=tokenizer.pad_token_id))\n",
    "    val_loader = DataLoader(NERDataset(X_val, y_val, tokenizer, label2id), batch_size=bs, shuffle=False, collate_fn=partial(collate_fn, pad_token_id=tokenizer.pad_token_id))\n",
    "    model = train(model, train_loader, val_loader, epochs=epochs, lr=lr, finetune=finetune)\n",
    "\n",
    "    for lang, data in getTestDatasets(val_langs=val_langs).items():\n",
    "        X_test, y_test = data\n",
    "        test_loader = DataLoader(NERDataset(X_test, y_test, tokenizer, label2id), batch_size=bs, shuffle=False, collate_fn=partial(collate_fn, pad_token_id=tokenizer.pad_token_id))\n",
    "        test_loss, true_tags, pred_tags, _ = validate(model, test_loader, evaluate=True, lang=lang)\n",
    "\n",
    "# train_test([\"en\"], [\"en\", \"de\"], model_path = \"/fp/projects01/ec30/models/xlm-roberta-base/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50eb597-87bc-4f5c-9d69-29f26177e275",
   "metadata": {},
   "source": [
    "# Part 1: Finetuning Multilingual Transformers on English)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f07c7b-2e33-4bb1-b051-470ae7a90377",
   "metadata": {},
   "source": [
    "## 1A: Different Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb1039-fa64-4afe-b0f7-ee254250d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at /fp/projects01/ec30/models/xlm-roberta-base/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_test([\"en\"], [\"en\"], model_path = \"/fp/projects01/ec30/models/xlm-roberta-base/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b3140-1ecf-4f63-b4bf-a640785684fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test([\"en\"], [\"en\"], model_path = \"/fp/projects01/ec30/models/bert-base-multilingual-cased/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd11e9-81ad-4e10-81b2-ed69e3e6c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bca664-bed7-45ff-b1be-73d20a6d44ef",
   "metadata": {},
   "source": [
    "## 1B: Finetuning and freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15807def-fd8a-449f-bb9d-5845664dce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for finetune in [True, False]:\n",
    "    print(\"Finetune: \", finetune)\n",
    "    train_test([\"en\"], [\"en\"], model_path = \"/fp/projects01/ec30/models/xlm-roberta-base/\",  finetune=finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ced2a0-6f39-41e7-8206-454656e167b2",
   "metadata": {},
   "source": [
    "## 1C:  evaluate the finetuned models on the development set in each language (English, Italian, Afrikaans, Swahili, and German)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585556c5-bbae-4806-ac41-d80e48465be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test([\"en\"],[\"en\", \"it\", \"af\", \"sw\", \"de\"], model_path = \"/fp/projects01/ec30/models/xlm-roberta-base/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563bf78f-70cb-485d-b06a-e11344755924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40b2d002-c13e-4213-9f3a-ffa0219534da",
   "metadata": {},
   "source": [
    "# Part 2: Finetuning Multilingual Transformers on Mixture of Languages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6194fc-ab71-42a3-ada5-3338242d18c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test([\"en\",  \"it\"],[\"en\", \"it\", \"af\", \"sw\", \"de\"], model_path = \"/fp/projects01/ec30/models/xlm-roberta-base/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87c4ef-97d5-4149-9165-7946b5f5aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test([\"en\",  \"de\"],[\"en\", \"it\", \"af\", \"sw\", \"de\"], model_path = \"/fp/projects01/ec30/models/xlm-roberta-base/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f9ad70-3bbb-4498-8e41-54882738bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test([\"en\", \"it\", \"de\"],[\"en\", \"it\", \"af\", \"sw\", \"de\"], model_path = \"/fp/projects01/ec30/models/xlm-roberta-base/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06226c6d-7a83-44a4-96d3-f205c2556e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddd9cee8-5e81-47ea-88e5-2b9bd68b9972",
   "metadata": {},
   "source": [
    "# Part 3:A Surprise Language "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
